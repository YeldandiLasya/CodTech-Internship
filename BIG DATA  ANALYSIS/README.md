
# Big Data Analysis using PySpark | TASK-1

ğŸ¯ Objective
The goal of this task is to apply Big Data analytics techniques using PySpark on a real-world retail dataset consisting of over 400,000 transaction records, in order to:

- Clean and process customer transaction data
- Analyze product categories, customer segments, and sales performance
- Understand customer ratings, purchase behavior, and feedback patterns
- Demonstrate scalability and efficiency using PySpark for large datasets

## ğŸ“ Dataset

- Source: [Retail Analysis Large Dataset (Kaggle)](https://www.kaggle.com/datasets/sahilprajapati143/retail-analysis-large-dataset)
- File Used: `new_retail_data.csv`
- Sample size: ~100,000+ records
- Fields: Customer info, Transaction details, Product categories, Amount, Ratings, Time, etc.

## ğŸš€ Tools & Technologies

- ğŸ Python
- ğŸ”¥ PySpark (Apache Spark for Python)
- ğŸ“Š Matplotlib & Seaborn (for visualizations)
- ğŸ§  Pandas (for plotting with Spark-converted DataFrames)

## ğŸ“ˆ Key Features / Analysis

The project performs the following analyses using PySpark:

1. **Total Revenue by Country**  
2. **Top-Selling Product Categories**
3. **Average Spend by Customer Segment**
4. **Monthly Sales Trend**
5. **Gender-based Spending Analysis**
6. **Ratings Distribution**
