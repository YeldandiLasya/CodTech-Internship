
# Big Data Analysis using PySpark | TASK-1

## 🎯 Objective
The goal of this task is to apply Big Data analytics techniques using PySpark on a real-world retail dataset consisting of over 400,000 transaction records, in order to:

- Clean and process customer transaction data
- Analyze product categories, customer segments, and sales performance
- Understand customer ratings, purchase behavior, and feedback patterns
- Demonstrate scalability and efficiency using PySpark for large datasets

## 📁 Dataset

- Source: [Retail Analysis Large Dataset (Kaggle)](https://www.kaggle.com/datasets/sahilprajapati143/retail-analysis-large-dataset)
- File Used: `new_retail_data.csv`
- Sample size: ~100,000+ records
- Fields: Customer info, Transaction details, Product categories, Amount, Ratings, Time, etc.

## 🚀 Tools & Technologies

- 🐍 Python
- 🔥 PySpark (Apache Spark for Python)
- 📊 Matplotlib & Seaborn (for visualizations)
- 🧠 Pandas (for plotting with Spark-converted DataFrames)

## 📊 Data Analysis Performed

1.Data Cleaning
- Dropped rows with nulls in essential fields (Amount, Ratings, Country, etc.)
- Converted column types (e.g., casting Amount and Ratings as numeric)
Analytical Insights
Total revenue by country
Most purchased product categories
Average spending by customer segment and gender

Monthly sales trend

Distribution of customer ratings

Relationship between feedback types and order status

Visualizations

Bar plots for revenue and categories

Line charts for monthly trends

Rating distribution plots

Gender and segment-based comparison charts

⭐ Key Highlights
Processed and visualized 400,000+ retail transactions

Identified high-revenue countries and top-selling categories

Uncovered premium customers have the highest average spend

Found sales peaks in festive months (e.g., November, December)

Clear rating bias toward 4 and 5 stars

Real-time feedback and order statuses show business performance
